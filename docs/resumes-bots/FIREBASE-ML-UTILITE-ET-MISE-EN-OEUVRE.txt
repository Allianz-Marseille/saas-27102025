================================================================================
  FIREBASE MACHINE LEARNING — Utilité et mise en œuvre pour les bots
================================================================================

Ce fichier explique si les fonctionnalités ML de Firebase peuvent être utiles
pour les bots (Bob, Nina, Sinistro) et comment les mettre en œuvre techniquement
dans le contexte actuel (Next.js, Firebase Auth/Firestore, OpenAI).


1. CE QUE FIREBASE PROPOSE EN MACHINE LEARNING
--------------------------------------------------------------------------------

Firebase propose deux volets ML :

  A) Firebase Machine Learning (Firebase ML)
     - APIs prêtes à l’emploi (inférence dans le cloud) :
       • Reconnaissance de texte (OCR) — plus de 100 langues
       • Étiquetage d’images (image labeling) — milliers de labels
       • Reconnaissance de points d’intérêt (landmarks)
     - Déploiement de modèles personnalisés TensorFlow Lite pour apps mobiles
       (téléchargement dynamique, A/B testing, Remote Config).

  B) ML Kit (SDK séparé)
     - Modèles on-device (mobile) : texte, visages, codes-barres, étiquetage,
       détection d’objets, identification de langue, traduction, etc.
     - Certaines capacités GenAI (résumé, correction, réécriture, description
       d’image, reconnaissance vocale) — selon plateforme et offre.

  Limitation importante pour votre stack
  - La doc officielle Firebase ML / ML Kit cible les applications mobiles
    (iOS, Android). Il n’y a pas de SDK Firebase ML pour Node.js ou le web.
  - Pour un backend Next.js : les cas d’usage “reconnaissance de texte” et
    “étiquetage d’images” sont couverts par Google Cloud Vision API (même
    technologie sous-jacente), utilisable depuis Node.js avec @google-cloud/vision.


2. UTILITÉ POUR CHAQUE BOT
--------------------------------------------------------------------------------

  BOB (Santé & Prévoyance)
  - Uploads : liasses, 2035, attestations (souvent en image/PDF).
  - Utilité ML : OCR sur les images/PDF pour extraire le texte avant ou en
    complément de l’envoi au LLM. Permet de mieux interroger le RAG ou de
    pré-remplir des champs (chiffres, noms de caisses).
  - Verdict : utile si vous voulez structurer davantage l’analyse des
    documents (ex. extraction de champs fixes). Actuellement le flux
    envoie déjà les images au modèle (Vision) ; l’OCR dédié peut améliorer
    la précision pour les formulaires et tableaux.

  NINA (Secrétaire)
  - Analyse de documents (PDF, Word, Excel), correction de textes, comparaison
    de devis. Pas de RAG, prompt seul.
  - Utilité ML :
    • OCR (reconnaissance de texte) : très pertinent pour PDF scannés ou
      photos de documents, avant résumé/analyse par le LLM.
    • Étiquetage d’images : secondaire (détecter “document”, “tableau” peut
      aider à router le traitement, mais pas indispensable).
  - Verdict : utile, surtout l’OCR pour tout document non déjà en texte
    (PDF image, photos de courriers/devis).

  SINISTRO (Sinistres)
  - Analyse de constats amiables (souvent fournis en image/PDF).
  - Utilité ML : OCR très pertinent pour extraire zones du constat (véhicules,
    croquis, cases cochées, textes manuscrits) et les passer au LLM de façon
    structurée. Peut améliorer la qualité de la qualification (IRSA, IRSI,
    droit commun).
  - Verdict : très utile pour automatiser et fiabiliser l’analyse des
    constats.

  Synthèse
  - Oui, les fonctionnalités de type “Firebase ML” (OCR + éventuellement
    image labeling) peuvent être utiles, en particulier pour Nina et
    Sinistro, et pour Bob si vous poussez l’analyse de documents.
  - Dans votre architecture actuelle (Next.js, pas d’app mobile), la mise
    en œuvre passe par Google Cloud Vision API (et éventuellement Document
    AI pour des formulaires structurés), pas par le SDK Firebase ML mobile.


3. MISE EN ŒUVRE TECHNIQUE (NEXT.JS / FIREBASE)
--------------------------------------------------------------------------------

  Option recommandée : Google Cloud Vision API côté backend

  Pourquoi pas Firebase ML directement ?
  - Firebase ML expose ces capacités via des SDK iOS/Android. Pour un
    backend Node.js (route API Next.js), il faut utiliser l’API sous-jacente
    (Cloud Vision), avec le même projet Google Cloud / Firebase.

  Étapes techniques

  1) Activer l’API et les credentials
     - Dans Google Cloud Console (projet lié à votre app Firebase) :
       activer “Cloud Vision API”.
     - Créer une clé de compte de service (JSON) ou réutiliser celle de
       Firebase Admin, avec le rôle nécessaire (ex. “Cloud Vision API User”).
     - En local / déploiement : GOOGLE_APPLICATION_CREDENTIALS pointant vers
       le fichier JSON, ou configuration équivalente (Secret Manager, etc.).

  2) Dépendance Node.js
     - npm install @google-cloud/vision

  3) Utilisation dans une route API ou un module partagé
     - Créer un client Vision (ImageAnnotatorClient).
     - Pour l’OCR : textDetection() ou documentTextDetection() (pour blocs,
       paragraphes, tableaux).
     - Pour l’étiquetage : labelDetection().
     - Entrée : buffer d’image (ou base64) que vous avez déjà côté API
       (uploads, images converties depuis PDF, etc.).

  4) Intégration dans le flux des bots
     - Nina : avant d’envoyer une image/PDF au LLM, appeler l’OCR ; envoyer
       le texte extrait en plus (ou à la place) de l’image pour les PDF
       scannés, pour de meilleurs résumés/analyses.
     - Sinistro : pour chaque image de constat amiable, lancer l’OCR (idéalement
       documentTextDetection pour garder la structure), puis injecter le
       texte structuré dans le contexte du prompt Sinistro (ou en entrée
       structurée).
     - Bob : idem, optionnel, pour les attestations/liasses/2035 en image :
       OCR puis texte ajouté au contexte ou au message.

  5) Coûts et quotas
     - Vision API : quota gratuit mensuel (ex. 1 000 premières unités pour
       certaines méthodes). Au-delà, facturation au volume. Consulter la
       page tarifaire Cloud Vision.

  Si vous ajoutez plus tard une app mobile (React Native / native)
  - Vous pourrez alors utiliser Firebase ML / ML Kit directement dans
    l’app pour de l’OCR ou du labelling on-device (rapide, hors ligne),
    tout en gardant Vision API côté backend Next.js pour les flux serveur.


4. RÉSUMÉ
--------------------------------------------------------------------------------

  - Les fonctionnalités “machine learning” de Firebase (OCR, étiquetage
    d’images) sont utiles pour Nina, Sinistro et Bob, surtout l’OCR sur
    documents et constats.
  - Dans un projet Next.js sans app mobile, il ne faut pas implémenter
    Firebase ML côté serveur : utiliser Google Cloud Vision API (@google-cloud/vision)
    dans vos routes API ou lib, avec le même projet GCP que Firebase.
  - Mise en œuvre : activer Cloud Vision API, credentials, npm install
    @google-cloud/vision, puis appeler textDetection/documentTextDetection
    (et optionnellement labelDetection) dans le pipeline de traitement des
    images/PDF des trois bots, avant ou en complément de l’appel OpenAI.

================================================================================
